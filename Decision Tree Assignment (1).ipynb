{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjlIBkmhAVTHToeh9sjJkx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. What is a Decision Tree, and how does it work\t?\n","- A Decision Tree is a flowchart-like model used for decision-making and predicting outcomes. It splits data into branches based on feature values, ending in leaves that represent results (like a class or value).\n","\n","- It works:\n","\n","1. Starts at the root (top).\n","2. Chooses the best feature to split data (using Gini, Entropy, etc.).\n","3. Splits data into branches.\n","4. Repeats this for each branch until a stopping condition is met (like pure nodes or max depth).\n","\n","- It's used in classification and regression tasks.\n"],"metadata":{"id":"H9STvUAG6m1K"}},{"cell_type":"markdown","source":["2. What are impurity measures in Decision Trees\t?\n","- Impurity measures in Decision Trees show how mixed the data is at a node.\n","\n","- Common types:\n","\n","1. Gini Impurity - Measures how often a randomly chosen element would be incorrectly labeled.\n","2. Entropy - Measures the level of disorder or uncertainty.\n","3. Classification Error - Measures the fraction of misclassified examples.\n","\n","- Lower impurity = better split.\n"],"metadata":{"id":"3mhnLzuY7IkC"}},{"cell_type":"markdown","source":["3. What is the mathematical formula for Gini Impurity\t?\n","- The mathematical formula for Gini Impurity is:\n","- Gini = 1-∑pi2\n","- Where:\n","- pi = proportion of class i in the mode\n","- n =  NUmber in class\n","- It measures how often a randomly chosen element would be misclassified.\n"],"metadata":{"id":"uDm_4BF-7ptb"}},{"cell_type":"markdown","source":["4. What is the mathematical formula for Entropy\t?\n","- The mathematical formula for Entropy is:\n","- Entropy = - ∑pilog2(pi)\n","- Where\n","- pi = Proportion of class i in the mode\n","- n = number in class\n","- It measures the amount of uncertainty or disorder in the data.\n"],"metadata":{"id":"cxNkx9bN8kWi"}},{"cell_type":"markdown","source":["5. What is Information Gain, and how is it used in Decision Trees\t?\n","- Information Gain measures the reduction in entropy after a dataset is split on a feature.\n","- Use in Decision Trees:\n","It helps select the best feature to split—the one with the highest Information Gain."],"metadata":{"id":"DON5tZlC96Uh"}},{"cell_type":"markdown","source":["6. What is the difference between Gini Impurity and Entropy\t?\n","-  The difference between Gini Impurity and Entropy in a simpler, more direct way:\n","- Gini is simpler and quicker; Entropy is more precise but slower.\n","Both help pick the best feature to split a decision tree."],"metadata":{"id":"w5U5skRD-Hup"}},{"cell_type":"markdown","source":["7. What is the mathematical explanation behind Decision Trees?\n","- Decision Tree math in short:\n","\n","1. Calculate impurity (Gini or Entropy) at the root.\n","2. Try all splits on each feature.\n","3. Compute gain = impurity before split - impurity after split.\n","4. Pick the split with highest gain.\n","5. Repeat this process for each branch.\n","\n","- Goal: keep splitting to reduce impurity and build the best tree.\n"],"metadata":{"id":"E_50d0NV-izC"}},{"cell_type":"markdown","source":["8. What is Pre-Pruning in Decision Trees?\n","- Pre-Pruning is stopping the tree growth early during training to prevent overfitting.\n","\n","- It sets conditions like:\n","\n","1. Maximum tree depth\n","2. Minimum samples per node\n","3. Minimum impurity decrease\n","\n","- If these conditions are met, splitting stops, making the tree simpler and more general.\n"],"metadata":{"id":"mPIefTh8_E-B"}},{"cell_type":"markdown","source":["9. What is Post-Pruning in Decision Trees?\n","- Post-Pruning means growing a full tree first, then **cutting back** (removing) branches that don't improve performance.\n","\n","- It helps reduce overfitting by simplifying the tree after it's fully built.\n"],"metadata":{"id":"gTLcNudZ_hWh"}},{"cell_type":"markdown","source":["10. What is the difference between Pre-Pruning and Post-Pruning\t?\n","- The difference without a table :\n","\n","- Pre-Pruning stops the tree from growing too much during training by setting limits like maximum depth or minimum samples per node. It prevents complexity early but might stop too soon and underfit.\n","\n","- Post-Pruning lets the tree grow fully first, then cuts back branches that don't improve performance. It usually gives better accuracy but takes more time since pruning happens after training.\n","\n","- So, pre-pruning controls tree size before it grows, while post-pruning simplifies the tree after it's fully built.\n"],"metadata":{"id":"RHevI4Ub_vXw"}},{"cell_type":"markdown","source":["11. What is a Decision Tree Regressor ?\n","- A Decision Tree Regressor is a decision tree used for predicting continuous values (like prices or temperatures) instead of categories.\n","\n","- It works by splitting the data into branches based on feature values, and at each leaf, it predicts the average value of the target in that subset.\n"],"metadata":{"id":"lBvfaux3AHJJ"}},{"cell_type":"markdown","source":["12. What are the advantages and disadvantages of Decision Trees\t?\n","- Advantages of Decision Trees:\n","\n","1. Easy to understand and interpret\n","2. Handles both numerical and categorical data\n","3. Requires little data preprocessing\n","4. Can model nonlinear relationships\n","5. Useful for both classification and regression\n","\n","- Disadvantages of Decision Trees:\n","\n","1. Prone to overfitting, especially with deep trees\n","2. Can be unstable (small changes in data can change the tree)\n","3. May be biased towards features with more levels\n","4.  Less accurate compared to some other models like Random Forests or Boosting methods\n"],"metadata":{"id":"3EwjjY6AAW4J"}},{"cell_type":"markdown","source":["13.  How does a Decision Tree handle missing values\t?\n","- A Decision Tree can handle missing values in a few ways:\n","\n","1. Ignore missing values during splitting by using only available data.\n","2. Use surrogate splits: find another feature that closely mimics the primary split to decide where to send missing data.\n","3. Assign missing values to the most common branch or distribute them proportionally based on training data.\n","\n","- These methods help the tree still make good decisions despite missing data.\n"],"metadata":{"id":"RTcw-xrkBBop"}},{"cell_type":"markdown","source":["14. How does a Decision Tree handle categorical features?\n","- A Decision Tree handles categorical features by splitting the data based on the categories:\n","\n","1. 7It can split by checking if the feature equals a specific category (e.g., “Color = Red”).\n","2.  Or it can group categories into subsets (e.g., “Color in {Red, Blue} vs. others”).\n","3.  The tree finds the best way to split categories to reduce impurity.\n","- No special encoding is needed; the tree naturally works with categorical data.\n"],"metadata":{"id":"KfA4OEtUBmxI"}},{"cell_type":"markdown","source":["15. What are some real-world applications of Decision Trees?\n","- Some real-world applications of Decision Trees include:\n","\n","1. Customer churn prediction: Identifying which customers might leave a service.\n","2. Medical diagnosis: Classifying diseases based on symptoms.\n","3. Credit scoring: Assessing loan risk based on applicant data.\n","4. Fraud detection: Spotting suspicious transactions.\n","5. Marketing: Targeting customers for campaigns based on behavior.\n","6. Manufacturing: Predicting machine failures or quality issues.\n","- They're popular because they're easy to interpret and implement.\n"],"metadata":{"id":"ML_CDgkjB8GA"}},{"cell_type":"code","source":["#16.  Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the Decision Tree Classifier\n","clf = DecisionTreeClassifier(random_state=42)\n","\n","# Train the model\n","clf.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = clf.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Model Accuracy: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uA6mMHu2CUIo","executionInfo":{"status":"ok","timestamp":1748673163626,"user_tz":-330,"elapsed":5894,"user":{"displayName":"Sanket Khatik","userId":"10923466111963992924"}},"outputId":"b423b312-f50e-413a-dfef-723f08b5bba9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 1.00\n"]}]},{"cell_type":"code","source":["#17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Initialize Decision Tree with Gini criterion\n","clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n","\n","# Train the model\n","clf.fit(X, y)\n","\n","# Print feature importances\n","for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n","    print(f\"{feature}: {importance:.4f}\")\n","\n"],"metadata":{"id":"SiVNdMzyD27w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize Decision Tree with Entropy criterion\n","clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n","\n","# Train the model\n","clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = clf.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Model Accuracy: {accuracy:.2f}\")\n","\n"],"metadata":{"id":"pjIq5zZZEJcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#19.Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)?\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Load California housing dataset\n","data = fetch_california_housing()\n","X = data.data\n","y = data.target\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize Decision Tree Regressor\n","regressor = DecisionTreeRegressor(random_state=42)\n","\n","# Train the model\n","regressor.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = regressor.predict(X_test)\n","\n","# Calculate Mean Squared Error\n","mse = mean_squared_error(y_test, y_pred)\n","\n","print(f\"Mean Squared Error: {mse:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TGnf9W7YEWXw","executionInfo":{"status":"ok","timestamp":1748673665869,"user_tz":-330,"elapsed":3091,"user":{"displayName":"Sanket Khatik","userId":"10923466111963992924"}},"outputId":"11e67fe8-cc5c-41b1-fab4-6a8b2057f64e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error: 0.4952\n"]}]},{"cell_type":"code","source":["#20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier, export_graphviz\n","import graphviz\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Train Decision Tree Classifier\n","clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(X, y)\n","\n","# Export the tree to DOT format\n","dot_data = export_graphviz(\n","    clf,\n","    out_file=None,\n","    feature_names=iris.feature_names,\n","    class_names=iris.target_names,\n","    filled=True,\n","    rounded=True,\n","    special_characters=True\n",")\n","\n","# Visualize the tree using graphviz\n","graph = graphviz.Source(dot_data)\n","graph.render(\"decision_tree_iris\")  # Saves the tree as PDF file\n","graph.view()  # Opens the visualization\n"],"metadata":{"id":"PPpAeTXrEnKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#21.  Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Decision Tree with max depth = 3\n","clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n","clf_limited.fit(X_train, y_train)\n","y_pred_limited = clf_limited.predict(X_test)\n","accuracy_limited = accuracy_score(y_test, y_pred_limited)\n","\n","# Fully grown Decision Tree (no max_depth limit)\n","clf_full = DecisionTreeClassifier(random_state=42)\n","clf_full.fit(X_train, y_train)\n","y_pred_full = clf_full.predict(X_test)\n","accuracy_full = accuracy_score(y_test, y_pred_full)\n","\n","print(f\"Accuracy with max depth 3: {accuracy_limited:.2f}\")\n","print(f\"Accuracy with fully grown tree: {accuracy_full:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mHchO8TqE1Io","executionInfo":{"status":"ok","timestamp":1748673756820,"user_tz":-330,"elapsed":65,"user":{"displayName":"Sanket Khatik","userId":"10923466111963992924"}},"outputId":"07959f1f-9e42-41f2-e183-94317dd89953"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with max depth 3: 1.00\n","Accuracy with fully grown tree: 1.00\n"]}]},{"cell_type":"code","source":["#22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split dataset into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Decision Tree with min_samples_split=5\n","clf_min_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n","clf_min_split.fit(X_train, y_train)\n","y_pred_min_split = clf_min_split.predict(X_test)\n","accuracy_min_split = accuracy_score(y_test, y_pred_min_split)\n","\n","# Default Decision Tree\n","clf_default = DecisionTreeClassifier(random_state=42)\n","clf_default.fit(X_train, y_train)\n","y_pred_default = clf_default.predict(X_test)\n","accuracy_default = accuracy_score(y_test, y_pred_default)\n","\n","print(f\"Accuracy with min_samples_split=5: {accuracy_min_split:.2f}\")\n","print(f\"Accuracy with default parameters: {accuracy_default:.2f}\")\n"],"metadata":{"id":"hhTtWvBnFAlB","executionInfo":{"status":"ok","timestamp":1748673810466,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sanket Khatik","userId":"10923466111963992924"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Without scaling\n","clf_unscaled = DecisionTreeClassifier(random_state=42)\n","clf_unscaled.fit(X_train, y_train)\n","y_pred_unscaled = clf_unscaled.predict(X_test)\n","accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n","\n","# With feature scaling\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","clf_scaled = DecisionTreeClassifier(random_state=42)\n","clf_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = clf_scaled.predict(X_test_scaled)\n","accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","print(f\"Accuracy without scaling: {accuracy_unscaled:.2f}\")\n","print(f\"Accuracy with scaling: {accuracy_scaled:.2f}\")\n","\n"],"metadata":{"id":"J_hIg3YhFZeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification ?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize Decision Tree classifier\n","dt = DecisionTreeClassifier(random_state=42)\n","\n","# Wrap it with OneVsRestClassifier\n","ovr = OneVsRestClassifier(dt)\n","\n","# Train the OvR model\n","ovr.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = ovr.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Model Accuracy (OvR): {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LnhYm9ZPFghU","executionInfo":{"status":"ok","timestamp":1748673940217,"user_tz":-330,"elapsed":126,"user":{"displayName":"Sanket Khatik","userId":"10923466111963992924"}},"outputId":"b6c25e7f-2f6c-4d7f-e2da-0505a4f1a866"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy (OvR): 1.00\n"]}]},{"cell_type":"code","source":["#25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Train Decision Tree Classifier\n","clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(X, y)\n","\n","# Display feature importances\n","for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n","    print(f\"{feature}: {importance:.4f}\")\n"],"metadata":{"id":"FoXDz_yKFtT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree?\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# Load dataset\n","data = fetch_california_housing()\n","X = data.data\n","y = data.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Decision Tree Regressor with max_depth=5\n","regressor_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n","regressor_limited.fit(X_train, y_train)\n","y_pred_limited = regressor_limited.predict(X_test)\n","mse_limited = mean_squared_error(y_test, y_pred_limited)\n","\n","# Unrestricted Decision Tree Regressor\n","regressor_full = DecisionTreeRegressor(random_state=42)\n","regressor_full.fit(X_train, y_train)\n","y_pred_full = regressor_full.predict(X_test)\n","mse_full = mean_squared_error(y_test, y_pred_full)\n","\n","print(f\"MSE with max_depth=5: {mse_limited:.4f}\")\n","print(f\"MSE with unrestricted tree: {mse_full:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o61DdYntF2zg","executionInfo":{"status":"ok","timestamp":1748674032503,"user_tz":-330,"elapsed":515,"user":{"displayName":"Sanket Khatik","userId":"10923466111963992924"}},"outputId":"4fd456bb-14d9-4c30-f7ff-3b27d6188156"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE with max_depth=5: 0.5245\n","MSE with unrestricted tree: 0.4952\n"]}]},{"cell_type":"code","source":["#27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy ?\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train initial tree to get effective alphas for pruning\n","clf = DecisionTreeClassifier(random_state=42)\n","path = clf.cost_complexity_pruning_path(X_train, y_train)\n","ccp_alphas = path.ccp_alphas\n","\n","train_scores = []\n","test_scores = []\n","\n","# Train trees with different ccp_alpha values and record accuracy\n","for ccp_alpha in ccp_alphas:\n","    clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n","    clf_pruned.fit(X_train, y_train)\n","    train_pred = clf_pruned.predict(X_train)\n","    test_pred = clf_pruned.predict(X_test)\n","    train_scores.append(accuracy_score(y_train, train_pred))\n","    test_scores.append(accuracy_score(y_test, test_pred))\n","\n","# Plot accuracy vs ccp_alpha\n","plt.figure(figsize=(8, 5))\n","plt.plot(ccp_alphas, train_scores, marker='o', label='Train Accuracy')\n","plt.plot(ccp_alphas, test_scores, marker='o', label='Test Accuracy')\n","plt.xlabel('ccp_alpha (Pruning parameter)')\n","plt.ylabel('Accuracy')\n","plt.title('Effect of Cost Complexity Pruning on Accuracy')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","\n"],"metadata":{"id":"sytYCFE7GDpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,Recall, and F1-Score?\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Decision Tree Classifier\n","clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate with precision, recall, and f1-score\n","report = classification_report(y_test, y_pred, target_names=iris.target_names)\n","print(report)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GvZWOyQnGuxY","executionInfo":{"status":"ok","timestamp":1748674245657,"user_tz":-330,"elapsed":57,"user":{"displayName":"Sanket Khatik","userId":"10923466111963992924"}},"outputId":"66282a20-2a30-4262-c68d-e561d501d81a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","      setosa       1.00      1.00      1.00        10\n","  versicolor       1.00      1.00      1.00         9\n","   virginica       1.00      1.00      1.00        11\n","\n","    accuracy                           1.00        30\n","   macro avg       1.00      1.00      1.00        30\n","weighted avg       1.00      1.00      1.00        30\n","\n"]}]},{"cell_type":"code","source":["#29.Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn ?\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Train Decision Tree Classifier\n","clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = clf.predict(X_test)\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# Plot confusion matrix using seaborn\n","plt.figure(figsize=(7,5))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n","            xticklabels=iris.target_names, yticklabels=iris.target_names)\n","plt.xlabel(\"Predicted\")\n","plt.ylabel(\"Actual\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()\n"],"metadata":{"id":"hzSLSIpTHAsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","\n","# Load Iris dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize Decision Tree Classifier\n","clf = DecisionTreeClassifier(random_state=42)\n","\n","# Define parameter grid\n","param_grid = {\n","    'max_depth': [2, 3, 4, 5, 6],\n","    'min_samples_split': [2, 4, 6, 8]\n","}\n","\n","# Setup GridSearchCV\n","grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n","\n","# Fit GridSearchCV\n","grid_search.fit(X_train, y_train)\n","\n","# Best parameters and best score\n","print(\"Best parameters:\", grid_search.best_params_)\n","print(f\"Best cross-validation accuracy: {grid_search.best_score_:.2f}\")\n","\n","# Evaluate on test data using the best estimator\n","best_clf = grid_search.best_estimator_\n","test_accuracy = best_clf.score(X_test, y_test)\n","print(f\"Test set accuracy: {test_accuracy:.2f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3JDJpuQ6HIfw","executionInfo":{"status":"ok","timestamp":1748674367121,"user_tz":-330,"elapsed":301,"user":{"displayName":"Sanket Khatik","userId":"10923466111963992924"}},"outputId":"d5d756e1-2470-4700-d8c8-814621ba30fd"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n","Best cross-validation accuracy: 0.94\n","Test set accuracy: 1.00\n"]}]}]}